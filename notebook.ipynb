{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nawel-Bellil/intelligent-travel-assistant-system-Llama/blob/main/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "![](./cover.jpg)\n",
        "\n",
        "Your airline's customer service team has been collecting chat data for years—thousands of conversations, each labeled with the user’s intent and an ideal response. Now, it's time to put that data to work.\n",
        "\n",
        "You've been tasked with fine-tuning a TinyLlama model to power the airline’s next-gen AI assistant. The goal? Given a user message, the model should predict the intent (like booking a flight, checking baggage status, or requesting special assistance) and generate a helpful, human-like response. Accurate intent detection is key since it helps the system understand what the customer wants, so it can respond appropriately and trigger downstream actions when needed.\n",
        "\n",
        "### The Data\n",
        "You'll work with a dataset of various travel query examples.\n",
        "\n",
        " Column | Description |\n",
        "|--------|-------------|\n",
        "| ```instruction``` | A user request from the Travel domain |\n",
        "| ```category``` | The high-level semantic category for the intent |\n",
        "| ```intent``` | The specific intent corresponding to the user instruction |\n",
        "| ```response``` | An example of an expected response from the virtual assistant |\n",
        "\n",
        "___\n",
        "### Update to Python 3.10\n",
        "\n",
        "Due to how frequently the libraries required for this project are updated, you'll need to update your environment to Python 3.10:\n",
        "\n",
        "1. In the workbook, click on \"Environment,\" in the top toolbar and select \"Session details\".\n",
        "\n",
        "2. In the workbook language dropdown, select \"Python 3.10\".\n",
        "\n",
        "3. Click \"Confirm\" and hit \"Done\" once the session is ready."
      ],
      "metadata": {
        "id": "40386fc5-3663-4cca-a90b-4a414e75a229"
      },
      "id": "40386fc5-3663-4cca-a90b-4a414e75a229",
      "cell_type": "markdown"
    },
    {
      "source": [
        "# First install the necessary packages\n",
        "!pip install -q -q -q trl==0.16.0\n",
        "!pip install -q -q -q tf-keras==2.19.0\n",
        "!pip install -q -q -q peft==0.14.0"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 10496,
        "lastExecutedAt": 1744731245893,
        "lastExecutedByKernel": "736e21fb-78f2-403a-b1d7-f8c43d0e0d00",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "# First install the necessary packages\n!pip install -q -q -q trl==0.16.0\n!pip install -q -q -q tf-keras==2.19.0\n!pip install -q -q -q peft==0.14.0",
        "outputsMetadata": {
          "0": {
            "height": 248,
            "type": "stream"
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de19a3c2-640a-4783-8cde-5a95e88d9bc2",
        "outputId": "d185e4f4-ec49-47c8-b05c-090a9a4a1602"
      },
      "id": "de19a3c2-640a-4783-8cde-5a95e88d9bc2",
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/335.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m327.7/335.7 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.7/335.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "source": [
        "# Import the required dependencies for this project\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from peft import LoraConfig\n",
        "\n",
        "from datasets import Dataset, load_dataset\n",
        "from collections import Counter, defaultdict\n",
        "import random\n",
        "import transformers\n",
        "import torch"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 8845,
        "lastExecutedAt": 1744731254740,
        "lastExecutedByKernel": "736e21fb-78f2-403a-b1d7-f8c43d0e0d00",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "# Import the required dependencies for this project\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom trl import SFTTrainer, SFTConfig\nfrom peft import LoraConfig\n\nfrom datasets import Dataset, load_dataset\nfrom collections import Counter, defaultdict\nimport random",
        "outputsMetadata": {
          "0": {
            "height": 416,
            "type": "stream"
          }
        },
        "id": "14082b66-5845-496b-888a-ebf28e4927a6"
      },
      "id": "14082b66-5845-496b-888a-ebf28e4927a6",
      "cell_type": "code",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "The code below loads the travel query dataset and reduces it from ~30k to ~50 records, keeping all intent types. This speeds up fine-tuning. Run it before starting, and feel free to experiment with it later!"
      ],
      "metadata": {
        "id": "d295fc56-0302-41a3-a1be-a863547fdef4"
      },
      "id": "d295fc56-0302-41a3-a1be-a863547fdef4",
      "cell_type": "markdown"
    },
    {
      "source": [
        "# First load the entire dataset\n",
        "ds = load_dataset('bitext/Bitext-travel-llm-chatbot-training-dataset', split=\"train\")\n",
        "\n",
        "# Group examples by intent\n",
        "random.seed(42)\n",
        "intent_groups = defaultdict(list)\n",
        "for record in ds:\n",
        "    intent = record[\"intent\"]\n",
        "    intent_groups[intent].append(record)\n",
        "\n",
        "# Determine how many samples per intent\n",
        "total_intents = len(intent_groups)\n",
        "samples_per_intent = 100 // total_intents\n",
        "\n",
        "# Sample from each intent\n",
        "balanced_subset = []\n",
        "for intent, examples in intent_groups.items():\n",
        "    sampled = random.sample(examples, min(samples_per_intent, len(examples)))\n",
        "    balanced_subset.extend(sampled)\n",
        "\n",
        "total_num_of_records = 50\n",
        "travel_chat_ds = Dataset.from_list(balanced_subset[:total_num_of_records])\n",
        "\n",
        "travel_chat_ds.to_pandas().head(3)"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 1611,
        "lastExecutedAt": 1744731790445,
        "lastExecutedByKernel": "736e21fb-78f2-403a-b1d7-f8c43d0e0d00",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "# First load the entire dataset\nds = load_dataset('bitext/Bitext-travel-llm-chatbot-training-dataset', split=\"train\")\n\n# Group examples by intent\nrandom.seed(42)\nintent_groups = defaultdict(list)\nfor record in ds:\n    intent = record[\"intent\"]\n    intent_groups[intent].append(record)\n\n# Determine how many samples per intent\ntotal_intents = len(intent_groups)\nsamples_per_intent = 100 // total_intents\n\n# Sample from each intent\nbalanced_subset = []\nfor intent, examples in intent_groups.items():\n    sampled = random.sample(examples, min(samples_per_intent, len(examples)))\n    balanced_subset.extend(sampled)\n\ntotal_num_of_records = 50    \ntravel_chat_ds = Dataset.from_list(balanced_subset[:total_num_of_records])\n\ntravel_chat_ds.to_pandas().head(3)",
        "outputsMetadata": {
          "0": {
            "height": 550,
            "type": "dataFrame",
            "tableState": {
              "customFilter": {
                "const": {
                  "type": "boolean",
                  "valid": true,
                  "value": true
                },
                "id": "f62c2997-7fde-4538-b6f9-ff8797ebb5ba",
                "nodeType": "const"
              }
            },
            "chartState": {
              "chartModel": {
                "modelType": "range",
                "chartId": "id-2v22o5ceq5l",
                "chartType": "groupedColumn",
                "chartThemeName": "datalabTheme",
                "chartOptions": {
                  "common": {
                    "animation": {
                      "enabled": true
                    }
                  }
                },
                "chartPalette": {
                  "fills": [
                    "#6568A0",
                    "#43D7A4",
                    "#4095DB",
                    "#FACC5F",
                    "#CAE279",
                    "#F08083",
                    "#5BCDF2",
                    "#F099DC",
                    "#965858",
                    "#7DB64F",
                    "#A98954"
                  ],
                  "strokes": [
                    "#6568A0",
                    "#43D7A4",
                    "#4095DB",
                    "#FACC5F",
                    "#CAE279",
                    "#F08083",
                    "#5BCDF2",
                    "#F099DC",
                    "#965858",
                    "#7DB64F",
                    "#A98954"
                  ],
                  "up": {
                    "fill": "#459d55",
                    "stroke": "#1e652e"
                  },
                  "down": {
                    "fill": "#ef5452",
                    "stroke": "#a82529"
                  },
                  "neutral": {
                    "fill": "#b5b5b5",
                    "stroke": "#575757"
                  },
                  "altUp": {
                    "fill": "#5090dc",
                    "stroke": "#2b5c95"
                  },
                  "altDown": {
                    "fill": "#ffa03a",
                    "stroke": "#cc6f10"
                  },
                  "altNeutral": {
                    "fill": "#b5b5b5",
                    "stroke": "#575757"
                  }
                },
                "cellRange": {
                  "rowStartIndex": null,
                  "rowStartPinned": null,
                  "rowEndIndex": null,
                  "rowEndPinned": null,
                  "columns": []
                },
                "switchCategorySeries": false,
                "suppressChartRanges": false,
                "unlinkChart": false,
                "version": "32.2.2"
              },
              "rangeChartModel": {
                "rangeColumns": [],
                "switchCategorySeries": false
              }
            }
          },
          "1": {
            "height": 50,
            "type": "dataFrame",
            "tableState": {
              "customFilter": {
                "const": {
                  "type": "boolean",
                  "valid": true,
                  "value": true
                },
                "id": "7da20b7d-655a-4080-87b0-f429ec0cf6a0",
                "nodeType": "const"
              },
              "quickFilterText": ""
            }
          },
          "3": {
            "height": 50,
            "type": "dataFrame",
            "tableState": {
              "customFilter": {
                "const": {
                  "type": "boolean",
                  "valid": true,
                  "value": true
                },
                "id": "fbf6cf80-78b8-4d90-a3bb-9cf4ea093b4f",
                "nodeType": "const"
              }
            }
          },
          "4": {
            "height": 50,
            "type": "dataFrame",
            "tableState": {
              "quickFilterText": "",
              "customFilter": {
                "const": {
                  "type": "boolean",
                  "valid": true,
                  "value": true
                },
                "id": "7da20b7d-655a-4080-87b0-f429ec0cf6a0",
                "nodeType": "const"
              }
            }
          }
        },
        "visualizeDataframe": false,
        "version": "ag-charts-v1",
        "id": "daa57ab5-d87c-42e6-808e-1e048ae3e0ad"
      },
      "id": "daa57ab5-d87c-42e6-808e-1e048ae3e0ad",
      "cell_type": "code",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Start the project with the dataset below\n",
        "\n",
        "display(travel_chat_ds)"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 10,
        "lastExecutedAt": 1744730539528,
        "lastExecutedByKernel": "292eaa60-9276-4354-ae84-d0a83579ad57",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "# Start the project with the dataset below\n\nprint(travel_chat_ds)",
        "outputsMetadata": {
          "0": {
            "height": 101,
            "type": "stream"
          }
        },
        "id": "7daaca1d-0c39-4ed5-8631-a61c30f00f36"
      },
      "id": "7daaca1d-0c39-4ed5-8631-a61c30f00f36",
      "cell_type": "code",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Start coding here\n",
        "# Use as many cells as you need"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 47,
        "lastExecutedAt": 1743611801015,
        "lastExecutedByKernel": "de62fdd2-76b4-4c07-928b-e06548466369",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "# Start coding here\n# Use as many cells as you need",
        "id": "07446785-915c-472d-971e-132f08ab9371"
      },
      "id": "07446785-915c-472d-971e-132f08ab9371",
      "cell_type": "code",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "\n",
        "model = \"PY007/TinyLlama-1.1B-Chat-v0.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "prompt = \"What are the values in open source projects?\"\n",
        "formatted_prompt = (\n",
        "    f\"### Human: {prompt}### Assistant:\"\n",
        ")\n",
        "\n",
        "\n",
        "sequences = pipeline(\n",
        "    formatted_prompt,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p = 0.7,\n",
        "    num_return_sequences=1,\n",
        "    repetition_penalty=1.1,\n",
        "    max_new_tokens=500,\n",
        ")\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")\n"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": null,
        "lastExecutedAt": null,
        "lastExecutedByKernel": null,
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": null,
        "outputsMetadata": {
          "8": {
            "height": 38,
            "type": "stream"
          }
        },
        "id": "9f9abcc4-5e02-4198-9b4c-874e465f9722"
      },
      "cell_type": "code",
      "id": "9f9abcc4-5e02-4198-9b4c-874e465f9722",
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "editor": "DataLab",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}