{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nawel-Bellil/intelligent-travel-assistant-system-Llama/blob/main/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40386fc5-3663-4cca-a90b-4a414e75a229",
      "metadata": {
        "id": "40386fc5-3663-4cca-a90b-4a414e75a229"
      },
      "source": [
        "![](./cover.jpg)\n",
        "\n",
        "Your airline's customer service team has been collecting chat data for years—thousands of conversations, each labeled with the user’s intent and an ideal response. Now, it's time to put that data to work.\n",
        "\n",
        "You've been tasked with fine-tuning a TinyLlama model to power the airline’s next-gen AI assistant. The goal? Given a user message, the model should predict the intent (like booking a flight, checking baggage status, or requesting special assistance) and generate a helpful, human-like response. Accurate intent detection is key since it helps the system understand what the customer wants, so it can respond appropriately and trigger downstream actions when needed.\n",
        "\n",
        "### The Data\n",
        "You'll work with a dataset of various travel query examples.\n",
        "\n",
        " Column | Description |\n",
        "|--------|-------------|\n",
        "| ```instruction``` | A user request from the Travel domain |\n",
        "| ```category``` | The high-level semantic category for the intent |\n",
        "| ```intent``` | The specific intent corresponding to the user instruction |\n",
        "| ```response``` | An example of an expected response from the virtual assistant |\n",
        "\n",
        "___\n",
        "### Update to Python 3.10\n",
        "\n",
        "Due to how frequently the libraries required for this project are updated, you'll need to update your environment to Python 3.10:\n",
        "\n",
        "1. In the workbook, click on \"Environment,\" in the top toolbar and select \"Session details\".\n",
        "\n",
        "2. In the workbook language dropdown, select \"Python 3.10\".\n",
        "\n",
        "3. Click \"Confirm\" and hit \"Done\" once the session is ready."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de19a3c2-640a-4783-8cde-5a95e88d9bc2",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "de19a3c2-640a-4783-8cde-5a95e88d9bc2",
        "outputId": "b71bba70-fcfc-42b9-eff8-3d4ac65e768c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.7/335.7 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# First install the necessary packages\n",
        "!pip install -q -q -q trl==0.16.0\n",
        "!pip install -q -q -q tf-keras==2.19.0\n",
        "!pip install -q -q -q peft==0.14.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14082b66-5845-496b-888a-ebf28e4927a6",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "14082b66-5845-496b-888a-ebf28e4927a6"
      },
      "outputs": [],
      "source": [
        "# Import the required dependencies for this project\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from peft import LoraConfig\n",
        "\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "from datasets import Dataset, load_dataset\n",
        "from collections import Counter, defaultdict\n",
        "import random\n",
        "import transformers\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d295fc56-0302-41a3-a1be-a863547fdef4",
      "metadata": {
        "id": "d295fc56-0302-41a3-a1be-a863547fdef4"
      },
      "source": [
        "The code below loads the travel query dataset and reduces it from ~30k to ~50 records, keeping all intent types. This speeds up fine-tuning. Run it before starting, and feel free to experiment with it later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daa57ab5-d87c-42e6-808e-1e048ae3e0ad",
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "da2c55ead6b2455da6ba200418aa1f98",
            "44786e295b6b4b57aa25200c786cb84b",
            "d97550d485df46ed94f6c08ba56d82a8"
          ]
        },
        "id": "daa57ab5-d87c-42e6-808e-1e048ae3e0ad",
        "outputId": "c2db160a-32f6-4bab-8a42-35f3e4998b9d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da2c55ead6b2455da6ba200418aa1f98",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/9.21k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44786e295b6b4b57aa25200c786cb84b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)-travel-llm-chatbot-training-dataset.csv:   0%|          | 0.00/19.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d97550d485df46ed94f6c08ba56d82a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/31658 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"travel_chat_ds\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"instruction\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"I'd like information about my checked baggage allowance, how can I find it?\",\n          \"i have to see the fucking checked baggage allowance can i get some help\",\n          \"I want to know about my checked baggage allowance, where can I find more information?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"intent\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"check_baggage_allowance\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"BAGGAGE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tags\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"BCIP\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"To retrieve your checked baggage allowance details, please follow these instructions:\\n\\n1. Visit {{WEBSITE_URL}} or launch the {{APP_NAME}} application.\\n2. Log in to your personal account.\\n3. Select the {{BOOKINGS_OPTION}} section.\\n4. Enter the required booking or flight information.\\n5. The specific baggage allowance for your trip will be displayed.\\n\\nShould you require additional help, do not hesitate to contact customer support via the {{APP_NAME}} app or on {{WEBSITE_URL}}.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-9ff7741f-0059-45a1-a34d-02188ae81c88\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instruction</th>\n",
              "      <th>intent</th>\n",
              "      <th>category</th>\n",
              "      <th>tags</th>\n",
              "      <th>response</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I'd like information about my checked baggage ...</td>\n",
              "      <td>check_baggage_allowance</td>\n",
              "      <td>BAGGAGE</td>\n",
              "      <td>BCIP</td>\n",
              "      <td>To retrieve your checked baggage allowance det...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i have to see the fucking checked baggage allo...</td>\n",
              "      <td>check_baggage_allowance</td>\n",
              "      <td>BAGGAGE</td>\n",
              "      <td>BCIQW</td>\n",
              "      <td>To determine your checked baggage allowance, p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I want to know about my checked baggage allowa...</td>\n",
              "      <td>check_baggage_allowance</td>\n",
              "      <td>BAGGAGE</td>\n",
              "      <td>BCI</td>\n",
              "      <td>To find details regarding your checked baggage...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ff7741f-0059-45a1-a34d-02188ae81c88')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9ff7741f-0059-45a1-a34d-02188ae81c88 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9ff7741f-0059-45a1-a34d-02188ae81c88');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-babf8e0d-6434-472b-8076-b6f5d06aa369\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-babf8e0d-6434-472b-8076-b6f5d06aa369')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-babf8e0d-6434-472b-8076-b6f5d06aa369 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                         instruction                   intent  \\\n",
              "0  I'd like information about my checked baggage ...  check_baggage_allowance   \n",
              "1  i have to see the fucking checked baggage allo...  check_baggage_allowance   \n",
              "2  I want to know about my checked baggage allowa...  check_baggage_allowance   \n",
              "\n",
              "  category   tags                                           response  \n",
              "0  BAGGAGE   BCIP  To retrieve your checked baggage allowance det...  \n",
              "1  BAGGAGE  BCIQW  To determine your checked baggage allowance, p...  \n",
              "2  BAGGAGE    BCI  To find details regarding your checked baggage...  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# First load the entire dataset\n",
        "ds = load_dataset('bitext/Bitext-travel-llm-chatbot-training-dataset', split=\"train\")\n",
        "\n",
        "# Group examples by intent\n",
        "random.seed(42)\n",
        "intent_groups = defaultdict(list)\n",
        "for record in ds:\n",
        "    intent = record[\"intent\"]\n",
        "    intent_groups[intent].append(record)\n",
        "\n",
        "# Determine how many samples per intent\n",
        "total_intents = len(intent_groups)\n",
        "samples_per_intent = 100 // total_intents\n",
        "\n",
        "# Sample from each intent\n",
        "balanced_subset = []\n",
        "for intent, examples in intent_groups.items():\n",
        "    sampled = random.sample(examples, min(samples_per_intent, len(examples)))\n",
        "    balanced_subset.extend(sampled)\n",
        "\n",
        "total_num_of_records = 50\n",
        "travel_chat_ds = Dataset.from_list(balanced_subset[:total_num_of_records])\n",
        "\n",
        "travel_chat_ds.to_pandas().head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7daaca1d-0c39-4ed5-8631-a61c30f00f36",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7daaca1d-0c39-4ed5-8631-a61c30f00f36",
        "outputId": "11078908-a3e5-4c49-8d5d-4407ec5a1ee0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['instruction', 'intent', 'category', 'tags', 'response'],\n",
              "    num_rows: 50\n",
              "})"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Start the project with the dataset below\n",
        "\n",
        "display(travel_chat_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07446785-915c-472d-971e-132f08ab9371",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "07446785-915c-472d-971e-132f08ab9371"
      },
      "outputs": [],
      "source": [
        "# Start coding here\n",
        "# Use as many cells as you need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f9abcc4-5e02-4198-9b4c-874e465f9722",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9f9abcc4-5e02-4198-9b4c-874e465f9722"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize model\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v0.1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uaCvaunrL-1u",
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "e78cd93cee164758b20206b81e51c4ac",
            "fb16199841614ec5b148e5eaf2ba5d7e",
            "f31bfe0ffd6147ac8739b356988ae8d2"
          ]
        },
        "id": "uaCvaunrL-1u",
        "outputId": "59531161-db21-4148-f73a-ea67cb9bb594"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e78cd93cee164758b20206b81e51c4ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/652 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb16199841614ec5b148e5eaf2ba5d7e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/4.40G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f31bfe0ffd6147ac8739b356988ae8d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/63.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load model with 8-bit quantization for memory efficiency\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "orquw83qMjDM",
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "b309eb0429ab4e0092944ca2abd115c8",
            "757def11858a4252a76b66b2d2d44168",
            "5c9d06ed3cd24e17a584a3afffd3d600",
            "1ce004d76c3241338899231d82989d7e",
            "33b5d2ae1b924523bda994a574a63b93"
          ]
        },
        "id": "orquw83qMjDM",
        "outputId": "b9c463f3-bea3-4e48-98fc-c7563e14d366"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b309eb0429ab4e0092944ca2abd115c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "757def11858a4252a76b66b2d2d44168",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c9d06ed3cd24e17a584a3afffd3d600",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ce004d76c3241338899231d82989d7e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33b5d2ae1b924523bda994a574a63b93",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jTk16q1iNr7q",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jTk16q1iNr7q"
      },
      "outputs": [],
      "source": [
        "def format_instruction(example):\n",
        "    instruction = example[\"instruction\"]\n",
        "    intent = example[\"intent\"]\n",
        "    response = example[\"response\"]\n",
        "\n",
        "    # Format: User query followed by intent classification and response\n",
        "    formatted_text = f\"\"\"### User: {instruction}\n",
        "\n",
        "### Intent: {intent}\n",
        "\n",
        "### Assistant: {response}\"\"\"\n",
        "\n",
        "    return {\"text\": formatted_text}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CJ12SwpwN0gY",
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "311681033b644dd798e83f57b37a634e"
          ]
        },
        "id": "CJ12SwpwN0gY",
        "outputId": "2b7d5ffe-bea2-44f2-be47-6b299503dd1e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "311681033b644dd798e83f57b37a634e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### User: I'd like information about my checked baggage allowance, how can I find it?\n",
            "\n",
            "### Intent: check_baggage_allowance\n",
            "\n",
            "### Assistant: To retrieve your checked baggage allowance details, please follow these instructions:\n",
            "\n",
            "1. Visit {{WEBSITE_URL}} or launch the {{APP_NAME}} application.\n",
            "2. Log in to your personal account.\n",
            "3. Select the {{BOOKINGS_OPTION}} section.\n",
            "4. Enter the required booking or flight information.\n",
            "5. The specific baggage allowance for your trip will be displayed.\n",
            "\n",
            "Should you require additional help, do not hesitate to contact customer support via the {{APP_NAME}} app or on {{WEBSITE_URL}}.\n"
          ]
        }
      ],
      "source": [
        "# Apply the formatting to our dataset\n",
        "formatted_ds = travel_chat_ds.map(format_instruction)\n",
        "\n",
        "# Display a sample to verify\n",
        "print(formatted_ds[0][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BWC02ks9N3gN",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BWC02ks9N3gN"
      },
      "outputs": [],
      "source": [
        "import trl\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j2j8rIyyN_L2",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "j2j8rIyyN_L2"
      },
      "outputs": [],
      "source": [
        "# Initialize LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                  # Rank of the update matrices\n",
        "    lora_alpha=32,         # Scaling factor\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zHwEMaonOCHt",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zHwEMaonOCHt"
      },
      "outputs": [],
      "source": [
        "# Initialize LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                  # Rank of the update matrices\n",
        "    lora_alpha=32,         # Scaling factor\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t7taKKcOOkHp",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "t7taKKcOOkHp",
        "outputId": "68779d3e-61cb-4740-fcf3-6a2ed5dbd0c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transformers.training_args\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "print(TrainingArguments.__module__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WdULBydSOYD_",
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "36e430006fa64147b7d1e94b36c842da",
            "a5322d9d326144caa4ca8bcd0021c596",
            "5e81b5dfe51c4fe685e2b5989d2c2abf",
            "99f270bfb4fe4560b55d95f911c67883"
          ]
        },
        "id": "WdULBydSOYD_",
        "outputId": "a40c6f77-454d-41c8-db3f-a01fd53f6e07"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36e430006fa64147b7d1e94b36c842da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Converting train dataset to ChatML:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5322d9d326144caa4ca8bcd0021c596",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying chat template to train dataset:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e81b5dfe51c4fe685e2b5989d2c2abf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99f270bfb4fe4560b55d95f911c67883",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ],
      "source": [
        "# Set up training arguments with wandb disabled\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    warmup_ratio=0.03,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    optim=\"adamw_torch\",\n",
        "    fp16=True,\n",
        "    report_to=\"none\"  # Disable wandb reporting\n",
        ")\n",
        "\n",
        "# Initialize the trainer with these updated arguments\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=formatted_ds,\n",
        "    peft_config=lora_config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bUTlgdgPjaZ",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9bUTlgdgPjaZ",
        "outputId": "4a97dad1-0d3f-4077-8243-cacfdfc4d179"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.16.0\n"
          ]
        }
      ],
      "source": [
        "import trl\n",
        "print(trl.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hfpOx2GvOGtE",
      "metadata": {
        "id": "hfpOx2GvOGtE"
      },
      "outputs": [],
      "source": [
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Save the final model\n",
        "trainer.model.save_pretrained(\"./travel-assistant-model\")\n",
        "tokenizer.save_pretrained(\"./travel-assistant-model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KzDVA6JkRBgI",
      "metadata": {
        "id": "KzDVA6JkRBgI"
      },
      "outputs": [],
      "source": [
        "!pip install -q bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V6ErAWW1TSk4",
      "metadata": {
        "id": "V6ErAWW1TSk4"
      },
      "outputs": [],
      "source": [
        "# Load the fine-tuned model\n",
        "from peft import PeftModel\n",
        "\n",
        "# Get the base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Load the PEFT adapter\n",
        "fine_tuned_model = PeftModel.from_pretrained(base_model, \"./travel-assistant-model\")\n",
        "\n",
        "# Test with a sample query\n",
        "test_query = \"I need to change my flight to tomorrow because of a family emergency.\"\n",
        "prompt = f\"\"\"### User: {test_query}\n",
        "\n",
        "### Intent:\"\"\"\n",
        "\n",
        "# Generate response\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = fine_tuned_model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    attention_mask=inputs[\"attention_mask\"],\n",
        "    max_new_tokens=150,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "model_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(model_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JvpHqo1RUCfY",
      "metadata": {
        "id": "JvpHqo1RUCfY"
      },
      "source": [
        "### This implementation covers all the required steps:\n",
        "\n",
        "- We load the TinyLlama model and tokenizer\n",
        "- We format the dataset to include both intent classification and response generation\n",
        "- We set up LoRA for efficient fine-tuning\n",
        "- We configure and run the SFT Trainer\n",
        "- We demonstrate how to generate responses with the fine-tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E4Bs0rORqNyg",
      "metadata": {
        "id": "E4Bs0rORqNyg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "HX-6IUbhT8eb",
      "metadata": {
        "id": "HX-6IUbhT8eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "360597ce-2967-430b-825f-692ed63840d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Notebook cleaned and saved to: /content/drive/MyDrive/Llama fine tuning/workspace/notebook.ipynb\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from nbformat import read, write\n",
        "import nbformat\n",
        "import os\n",
        "\n",
        "# Define the input and output paths\n",
        "input_path = \"/content/drive/MyDrive/Llama fine tuning/workspace/notebook1.ipynb\"\n",
        "output_path = \"/content/drive/MyDrive/Llama fine tuning/workspace/notebook.ipynb\"\n",
        "\n",
        "# Check if file exists before attempting to open\n",
        "if os.path.exists(input_path):\n",
        "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        nb = read(f, as_version=4)\n",
        "\n",
        "    # Clean up widget metadata\n",
        "    for cell in nb.cells:\n",
        "        if \"metadata\" in cell and \"widgets\" in cell[\"metadata\"]:\n",
        "            cell[\"metadata\"].pop(\"widgets\")\n",
        "\n",
        "    # Save the cleaned notebook\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        write(nb, f)\n",
        "\n",
        "    print(f\"Notebook cleaned and saved to: {output_path}\")\n",
        "else:\n",
        "    print(f\"File not found at: {input_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lecPGbADqHYw",
      "metadata": {
        "id": "lecPGbADqHYw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "editor": "DataLab",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}